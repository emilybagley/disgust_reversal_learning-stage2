---
title: "Model 2: mean regressive errors per reversal ~ feedback type"
format: gfm
fig-format: jpeg
---

<p>This file contains all model-agnostic tests run to test the effect of feedback type (fear, disgust, points) on regressive errors.</p>
<br>
Includes:
<p>
* initial skew assessment (and resulting skew transformation)
* initial hypothesis testing mixed effects model
* assessment of assumptions of this model (which was violated)
* resulting generalized mixed effects model
* assessing whether adding video-ratings differences (identified in video-rating analyses) moderates results
* sensitivity analysis (including generalized mixed effects models)
* final conclusions
</p>
<h3>Load in packages and data- in r and then in python </h3>
```{r, message=FALSE}
#| label: R packages
#| echo: true
#| code-fold: true
library(tidyverse, quietly=TRUE)
library(lme4)
library(emmeans)
library(DHARMa)
library('xlsx')
library('readxl')

task_summary <- read.csv("U:/Documents/Disgust learning project/github/disgust_reversal_learning-final/csvs/dem_vids_task_excluded.csv")

pvals_file = 'pvals/pvalsForPlotting.xlsx'
```


```{python}
#| label: Python packages
#| echo: true
#| code-fold: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import jsonlines
from functools import reduce
import statistics
import scipy.stats
import seaborn as sns
import math
import os
import json
import ast
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pingouin as pg
import warnings
from scipy.stats import ttest_rel
#from statannotations.Annotator import Annotator
from scipy.stats import skew
from statsmodels.stats.diagnostic import het_white
from sklearn.preprocessing import PowerTransformer
import statannot
from scipy.stats import ttest_ind
import itertools

warnings.simplefilter(action='ignore', category=FutureWarning)
pd.options.mode.copy_on_write = True

task_summary=pd.read_csv("U:/Documents/Disgust learning project/github/disgust_reversal_learning-final/csvs/dem_vids_task_excluded.csv")

pvals_file = 'pvals/pvalsForPlotting.xlsx'
```

<h3>Assess and correct for skewness in regressive error outcome</h3>
```{python}
#| label: Skewness
#| echo: true
#| code-fold: true
pt=PowerTransformer(method='yeo-johnson', standardize=False)
skl_yeojohnson=pt.fit(pd.DataFrame(task_summary.mean_regressive_er))
skl_yeojohnson=pt.transform(pd.DataFrame(task_summary.mean_regressive_er))
task_summary['regressive_er_transformed'] = pt.transform(pd.DataFrame(task_summary.mean_regressive_er))

fig, axes = plt.subplots(1, 2, sharey=True)
sns.histplot(data=task_summary, x="mean_regressive_er", ax=axes[0]) 
sns.histplot(data=task_summary['regressive_er_transformed'], ax=axes[1])
print('Regressive error skew: '+str(skew(task_summary.mean_regressive_er)))
```

<h3><b>Mixed effects model assumptions violated</b></h3>
In this case, the basic model (no random slopes or random intercepts, and no covariates) produced the best fit (indexed by BIC scores).
<p>But the model assumptions were violated</p>
```{python}
#| label: Mixed effects model - model selection
#| echo: true
#| code-fold: true
data=task_summary.reset_index()

formula = 'regressive_er_transformed ~ block_type'

basic_model=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)

#test which random effects to include
#feedback_randint=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'feedback_details': '0+feedback_details'}).fit(reml=False) CONVERGENCE WARNING
#fractals_randint=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'fractals': '0+fractals'}).fit(reml=False) CONVERGENCE WARNING
feedback_fractals_randint=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={"feedback_details": "0 + feedback_details", "fractals": "0 + fractals"}).fit(reml=False)

randslope=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', re_formula='~block_type').fit(reml=False)
feedback_randint_randslope=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'feedback_details': '0+feedback_details'}, re_formula='~block_type').fit(reml=False)
#feedback_fractals_randint_randslope=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'feedback_details': '0+feedback_details', "fractals": "0 + fractals"}, re_formula='~block_type').fit(reml=False) CONVERGENCE WARNING


bic=pd.DataFrame({'basic_model': [basic_model.bic], 
                   # 'feedback_andint': ['CONVERGENCE WARNING'], 
                  #  'fractals_randint': ['CONVERGENCE WARNING'],
                    'feedback_fractals_randint': [feedback_fractals_randint.bic],
                    'randslope': [randslope.bic],
                    'feedback_randint_randslope':[feedback_randint_randslope.bic],
                   # 'feedback_fractals_randint_randslope': ['CONVERGENCE WARNING']
                   })
win1=bic.sort_values(by=0, axis=1).columns[0]

##test which covariates to add -- Using the random effects which were best above (basic model in this case)
no_covariate=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_covariate=smf.mixedlm(formula+str('+prolific_sex'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
age_covariate=smf.mixedlm(formula+str('+prolific_age'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
digit_span_covariate=smf.mixedlm(formula+str('+digit_span'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_age_covariate=smf.mixedlm(formula+str('+prolific_sex+prolific_age'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_digit_span_covariate=smf.mixedlm(formula+str('+prolific_sex+digit_span'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
digit_span_age_covariate=smf.mixedlm(formula+str('+digit_span+prolific_age'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_age_digit_span_covariate=smf.mixedlm(formula+str('+prolific_sex+prolific_age+digit_span'), data, groups=data['participant_no'], missing='drop').fit(reml=False)

bic=pd.DataFrame({'no_covariate': [no_covariate.bic], 
                    'sex_covariate': [sex_covariate.bic], 
                    'age_covariate': [age_covariate.bic],
                    'digit_span_covariate': [digit_span_covariate.bic],
                    'sex_age_covariate': [sex_age_covariate.bic],
                    'sex_digit_span_covariate': [sex_digit_span_covariate.bic],
                    'digit_span_age_covariate': [digit_span_age_covariate.bic],
                    'sex_age_digit_span_covariate': [sex_age_digit_span_covariate.bic]})
win2=bic.sort_values(by=0, axis=1).columns[0]
print("Winning models: "+ win1 +" "+ win2)
```

<p>Shapiro-Wilk test of normality of residuals (violated)</p>
```{python}
#| label: Mixed effects model -shapiro-wilk
#| echo: true
#| code-fold: true
#chosen model
results=no_covariate

#shapiro-Wilk test of normality of residuals
labels = ["Statistic", "p-value"]
norm_res = stats.shapiro(results.resid)

for key, val in dict(zip(labels, norm_res)).items():
    print(key, val)
    ##if test is significant then the assumption is violated
        #is significant here
```

<p> White Lagrange multiplier Test for Heteroscedasticity (not violated) </p>
```{python}
#| label: Mixed effects model -white lagrange
#| echo: true
#| code-fold: true
#chosen model
##homoskedasticity of variance 
#White Lagrange Multiplier Test for Heteroscedasticity
het_white_res = het_white(results.resid, results.model.exog)

labels = ["LM Statistic", "LM-Test p-value", "F-Statistic", "F-Test p-value"]

for key, val in dict(zip(labels, het_white_res)).items():
    print(key, val)
    ##again, only violated if you get a significant p value
```


<h4>Run a generalized mixed effects model (done in R)</h4>
Model details:
<p>
* Gamma probability distribution and inverse link function
* random intercepts for individually selected feedback videos and by-participant random slopes
* no additional covariates
</p>
<p>This is the specification that produced the best fit (according to BIC)</p>

```{r}
#| label: generalized mixed effects model selection
#| echo: true
#| code-fold: true
#| warning: false

task_summary$pos_regressive_er <- task_summary$mean_regressive_er + 0.01 ##+0.01 as all values must be positive (i.e., can't have 0s)

#gamma_log <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=task_summary, family=Gamma(link="log"))
gamma_inverse <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=task_summary, family=Gamma(link="inverse"))
#gamma_identity <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=task_summary, family=Gamma(link="identity"))

invgaus_log <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=task_summary, family=inverse.gaussian(link="log"))
invgaus_inverse <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=task_summary, family=inverse.gaussian(link="inverse"))
#invgaus_identity <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=task_summary, family=inverse.gaussian(link="identity"))

bic_values <- c(
  BIC(gamma_inverse),
  BIC(invgaus_log),
  BIC(invgaus_inverse)
)
model_names <- c("Gamma (inverse)", "inverse gaussian (log)", "inverse gaussian (inverse)")

bic_df <- data.frame(Model = model_names, BIC = bic_values)
win1 <- bic_df[which.min(bic_df$BIC), ]$Model

basic_model <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=task_summary, family=Gamma(link="inverse"))

feedback_randint <- glmer(pos_regressive_er ~ block_type + (1|participant_no) + (1|feedback_details), data=task_summary, family=Gamma(link="inverse"))
fractals_randint <- glmer(pos_regressive_er ~ block_type + (1|participant_no) + (1|fractals), data=task_summary, family=Gamma(link="inverse"))
#feedback_fractals_randint <- glmer(pos_regressive_er ~ block_type + (1|participant_no) + (1|fractals) + (1|feedback_details), data=task_summary, family=Gamma(link="inverse"))

#randslope <- glmer(pos_regressive_er ~ block_type + (block_type|participant_no), data=task_summary, family=Gamma(link="inverse"))
feedback_randint_randslope <- glmer(pos_regressive_er ~ block_type + (block_type|participant_no) + (1|feedback_details), data=task_summary, family=Gamma(link="inverse"))
feedback_fractals_randint_randslope <- glmer(pos_regressive_er ~ block_type + (block_type|participant_no) + (1|feedback_details) + (1|fractals), data=task_summary, family=Gamma(link="inverse"))

bic_values <- c(
  BIC(basic_model),
  BIC(feedback_randint),
  BIC(fractals_randint),
  BIC(feedback_randint_randslope),
  BIC(feedback_fractals_randint_randslope)
)
model_names <- c("basic model", "feedback_randint", "fractals_randint", "feedback_randint_randslope", "feedback_fractals_randint_randslope")

bic_df <- data.frame(Model = model_names, BIC = bic_values)
win2 <- bic_df[which.min(bic_df$BIC), ]$Model

no_covariate <- feedback_randint_randslope

#sex_covariate <- glmer(pos_regressive_er ~ block_type + (block_type|participant_no) + (1|feedback_details) + prolific_sex, data=task_summary, family=Gamma(link="inverse"))
#age_covariate <- glmer(pos_regressive_er ~ block_type + (block_type|participant_no) + (1|feedback_details) + prolific_age, data=task_summary, family=Gamma(link="inverse"))
digit_span_covariate <- glmer(pos_regressive_er ~ block_type + (block_type|participant_no) + (1|feedback_details) + digit_span, data=task_summary, family=Gamma(link="inverse"))
#sex_age_covariate <- glmer(pos_regressive_er ~ block_type + (block_type|participant_no) + (1|feedback_details) + prolific_sex + prolific_age, data=task_summary, family=Gamma(link="inverse"))
#sex_digit_span_covariate <- glmer(pos_regressive_er ~ block_type + (block_type|participant_no) + (1|feedback_details) + prolific_sex + digit_span, data=task_summary, family=Gamma(link="inverse"))
digit_span_age_covariate <- glmer(pos_regressive_er ~ block_type + (block_type|participant_no) + (1|feedback_details) + digit_span + prolific_age, data=task_summary, family=Gamma(link="inverse"))
#sex_digit_span_age_covariate <- glmer(pos_regressive_er ~ block_type + (block_type|participant_no) + (1|feedback_details) + prolific_sex + prolific_age + digit_span, data=task_summary, family=Gamma(link="inverse"))

bic_values <- c(
  BIC(no_covariate),
  BIC(digit_span_covariate),
  BIC(digit_span_age_covariate)
)
model_names <- c("no_covariate", "digit_span_covariate", "digit_span_age_covariate")

bic_df <- data.frame(Model = model_names, BIC = bic_values)
win3 <- bic_df[which.min(bic_df$BIC), ]$Model

print(paste0("Winning models: ", win1, " ", win2," ",win3))
```


<p>Results from this model show <b>no effect of block-type</b> on regressive error rate.</p>
```{r}
#| label: generalized mixed effects model
#| echo: true
task_summary$pos_regressive_er <- task_summary$mean_regressive_er + 0.01 ##+0.01 as all values must be positive (i.e., can't have 0s)
generalized_model <- suppressMessages(glmer(pos_regressive_er ~ block_type + (block_type|participant_no) + (1|feedback_details), data=task_summary, family=Gamma(link="inverse")))
summary(generalized_model)
```


<p>Extract confidence intervals</p>
```{r}
#| label: confidence intervals
#| echo: true
print(confint.merMod(no_covariate, method='Wald'))
```


```{r}
#| label: save out pvalue
#| echo: false
pvalForPlotting <- suppressMessages(read_excel(pvals_file, col_names=TRUE))
pvalForPlotting[2,]$block_typeFear=summary(generalized_model)$coefficients["block_typeFear", "Pr(>|z|)"]
pvalForPlotting[2,]$block_typePoints=summary(generalized_model)$coefficients["block_typePoints", "Pr(>|z|)"]
rownames(pvalForPlotting)<-NULL
pvalsForPlotting<-as.data.frame(pvalForPlotting)
write.xlsx(pvalsForPlotting, file=pvals_file, row.names=FALSE)
```

<br>
<p>As this hypothesis test found a no difference between fear and disgust or disgust and points, we will compute a Bayes Factor to test the strength of the evidence for the null</p>

```{python}
#| label: bayes factor function
#| echo: true
#| code-fold: true
def bayes_factor(df, dependent_var, condition_1_name, condition_2_name):
    df=df[(df.block_type==condition_1_name)| (df.block_type==condition_2_name)][[dependent_var, 'block_type', 'participant_no']]
    df.dropna(inplace=True)
    df=df.pivot(index='participant_no', columns='block_type', values=dependent_var).reset_index()
    ttest=pg.ttest(df[condition_1_name], df[condition_2_name], paired=True)
    bf_null=1/float(ttest.BF10)
    return ttest, bf_null
```

<p>Firstly for disgust vs fear:</p>
```{python}
#| label: disgust vs fear bayes factor
#| echo: true
#| code-fold: true

ttest, bf_null = bayes_factor(task_summary, 'mean_regressive_er', 'Disgust', 'Fear')
#print("Disgust vs Fear BF01: " + bf_null)

print(f"Disgust vs Fear: BF01 = {bf_null}")
```

<br>
<p>Next for disgust vs points:</p>
```{python}
#| label: disgust vs points bayes factor
#| echo: true
#| code-fold: true

ttest, bf_null = bayes_factor(task_summary, 'mean_regressive_er', 'Disgust', 'Points')
#print("Disgust vs Points BF01: " + bf_null)

print(f"Disgust vs Points: BF01 = {bf_null}")
```

<br>
<p>We also look at fear vs points (which is not directly assessed by the model)</p>
```{python}
#| label: points vs fear ttest
#| echo: true
#| code-fold: true
ttest, bf_null = bayes_factor(task_summary, 'mean_regressive_er', 'Points', 'Fear')

print(f"Points vs Fear: T = {ttest['T'][0]}, CI95% = {ttest['CI95%'][0]}, p = {ttest['p-val'][0]}")
```

<p>And because the result is null, also get a Bayes factor: </p>
```{python}
#| label: points vs fear bayes factor
#| echo: true
#| code-fold: true
print(f"Points vs Fear: BF01 = {bf_null}")
```



```{python}
#| label: save out fear vs points pval
#| echo: false

pointsVsFear_pval = float(bayes_factor(task_summary, 'mean_regressive_er', 'Points', 'Fear')[0]['p-val'])
pvalForPlotting = pd.read_excel(pvals_file)
pvalForPlotting.loc[1, 'pointsVsFear']=pointsVsFear_pval
pvalForPlotting.to_excel(pvals_file, index=False)

```

<br>
<h3>Adding video ratings</h3>
Next, we will test whether this effect remains after video rating differences between fear and disgust have been controlled for. 
<p>As before, the mixed effects model violated assumptions, so a generalized mixed effects model must be run. </p>

```{python}
#| label: Mixed effects model video ratings - model selection
#| echo: true
#| code-fold: true

formula = 'regressive_er_transformed ~ block_type + valence_diff + arousal_diff + valence_habdiff'

basic_model=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)

#test which random effects to include
#feedback_randint=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'feedback_details': '0+feedback_details'}).fit(reml=False) CONVERGENCE WARNING
#fractals_randint=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'fractals': '0+fractals'}).fit(reml=False) CONVERGENCE WARNING
feedback_fractals_randint=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={"feedback_details": "0 + feedback_details", "fractals": "0 + fractals"}).fit(reml=False)
        #had to comment out because it does not converge and errors out

randslope=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', re_formula='~block_type').fit(reml=False)
feedback_randint_randslope=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'feedback_details': '0+feedback_details'}, re_formula='~block_type').fit(reml=False)
#feedback_fractals_randint_randslope=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'feedback_details': '0+feedback_details', "fractals": "0 + fractals"}, re_formula='~block_type').fit(reml=False)


bic=pd.DataFrame({'basic_model': [basic_model.bic], 
                  #  'feedback_andint': ['CONVERGENCE WRANING'], 
                   # 'fractals_randint': ['CONVERGENCE WARNING'],
                    'feedback_fractals_randint': [feedback_fractals_randint.bic], 
                    'randslope': [randslope.bic],
                    'feedback_randint_randslope':[feedback_randint_randslope.bic],
                  #  'feedback_fractals_randint_randslope': ['CONVERGENCE WARNING']
                  })
win1=bic.sort_values(by=0, axis=1).columns[0]

no_covariate=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_covariate=smf.mixedlm(formula+str('+prolific_sex'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
age_covariate=smf.mixedlm(formula+str('+prolific_age'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
digit_span_covariate=smf.mixedlm(formula+str('+digit_span'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_age_covariate=smf.mixedlm(formula+str('+prolific_sex+prolific_age'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_digit_span_covariate=smf.mixedlm(formula+str('+prolific_sex+digit_span'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
digit_span_age_covariate=smf.mixedlm(formula+str('+digit_span+prolific_age'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_age_digit_span_covariate=smf.mixedlm(formula+str('+prolific_sex+prolific_age+digit_span'), data, groups=data['participant_no'], missing='drop').fit(reml=False)

bic=pd.DataFrame({'no_covariate': [no_covariate.bic], 
                    'sex_covariate': [sex_covariate.bic], 
                    'age_covariate': [age_covariate.bic],
                    'digit_span_covariate': [digit_span_covariate.bic],
                    'sex_age_covariate': [sex_age_covariate.bic],
                    'sex_digit_span_covariate': [sex_digit_span_covariate.bic],
                    'digit_span_age_covariate': [digit_span_age_covariate.bic],
                    'sex_age_digit_span_covariate': [sex_age_digit_span_covariate.bic]})
win2=bic.sort_values(by=0, axis=1).columns[0]
print("Winning models: "+ win1 +" "+ win2)                   
```



<p>Shapiro-Wilk test of normality of residuals</p>
```{python}
#| label: mixed effects model shapiro-wilk - w video ratings
#| echo: true
#| code-fold: true

results=no_covariate
labels = ["Statistic", "p-value"]
norm_res = stats.shapiro(results.resid)

for key, val in dict(zip(labels, norm_res)).items():
    print(key, val)
```

<p> White Lagrange multiplier Test for Heteroscedasticity </p>
```{python}
#| label: Mixed effects model w video ratings -white lagrange
#| echo: true
#| code-fold: true
het_white_res = het_white(results.resid, results.model.exog)

labels = ["LM Statistic", "LM-Test p-value", "F-Statistic", "F-Test p-value"]

for key, val in dict(zip(labels, het_white_res)).items():
    print(key, val)
```


<br>
Generalized mixed effects model details:
<p>
* Gamma probability distribution and inverse link function
* random intercepts for fractals used as stimuli
* no additional covariates
</p>
<p>This is the specification that produced the best fit (according to BIC)</p>

```{r}
#| label: generalized model selection - w video ratings
#| echo: true
#| code-fold: true
#| warning: false
task_summary$pos_regressive_er <- task_summary$mean_regressive_er + 0.01 ##+0.01 as all values must be positive (i.e., can't have 0s)

##firstly we test whether model should use a gamma or inverse gaussian probability function
##and whether the link function should be identity or inverse
#gamma_log <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no), data=task_summary, family=Gamma(link="log"))
gamma_inverse <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no), data=task_summary, family=Gamma(link="inverse"))
#gamma_identity <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no), data=task_summary, family=Gamma(link="identity"))

invgaus_log <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no), data=task_summary, family=inverse.gaussian(link="log"))
invgaus_inverse <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no), data=task_summary, family=inverse.gaussian(link="inverse"))
#invgaus_identity <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no), data=task_summary, family=inverse.gaussian(link="identity"))

bic_values <- c(
  BIC(gamma_inverse),
  BIC(invgaus_log),
  BIC(invgaus_inverse)
)
model_names <- c("Gamma (inverse)", "inverse gaussian (log)", "inverse gaussian (inverse)")

bic_df <- data.frame(Model = model_names, BIC = bic_values)
win1 <- bic_df[which.min(bic_df$BIC), ]$Model

basic_model <-glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no), data=task_summary, family=Gamma(link="inverse"))

#feedback_randint <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no) + (1|feedback_details), data=task_summary, family=Gamma(link="inverse"))
fractals_randint <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no) + (1|fractals), data=task_summary, family=Gamma(link="inverse"))
#feedback_fractals_randint <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no) + (1|fractals) + (1|feedback_details), data=task_summary, family=Gamma(link="inverse"))

#randslope <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (block_type|participant_no), data=task_summary, family=Gamma(link="inverse"))
#feedback_randint_randslope <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (block_type|participant_no) + (1|feedback_details), data=task_summary, family=Gamma(link="inverse"))
#feedback_fractals_randint_randslope <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (block_type|participant_no) + (1|feedback_details) + (1|fractals), data=task_summary, family=Gamma(link="inverse"))

bic_values <- c(
  BIC(basic_model),
  BIC(fractals_randint)
)
model_names <- c("basic model", "fractals_randint")

bic_df <- data.frame(Model = model_names, BIC = bic_values)
win2 <- bic_df[which.min(bic_df$BIC), ]$Model

no_covariate <- fractals_randint #only model that converged
#sex_covariate <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no) + (1|fractals) + prolific_sex, data=task_summary, family=Gamma(link="inverse"))
#age_covariate <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no) + (1|fractals) + prolific_age, data=task_summary, family=Gamma(link="inverse"))
#digit_span_covariate <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no) + (1|fractals) + digit_span, data=task_summary, family=Gamma(link="inverse"))
#sex_age_covariate <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no) + (1|fractals) + prolific_sex + prolific_age, data=task_summary, family=Gamma(link="inverse"))
#sex_digit_span_covariate <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no) + (1|fractals) + prolific_sex + digit_span, data=task_summary, family=Gamma(link="inverse"))
#digit_span_age_covariate <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no) + (1|fractals) + digit_span + prolific_age, data=task_summary, family=Gamma(link="inverse"))
#sex_digit_span_age_covariate <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no) + (1|fractals) + prolific_sex + prolific_age + digit_span, data=task_summary, family=Gamma(link="inverse"))

win3 <- 'no_covariate'

print(paste0("Winning models: ", win1, " ", win2," ",win3))
```


<br>
<p>Adding video ratings has <b> no effect </b> on the results (i.e., there remains no effect of block-type on  regressive error rate)</p>
```{r}
#| label: generalized mixed effects model - w video ratings
#| echo: true
generalized_model <- glmer(pos_regressive_er ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no) + (1|fractals), data=task_summary, family=Gamma(link="inverse"))
summary(generalized_model)
```

<p>Extract confidence intervals</p>
```{r}
#| label: confidence intervals - video ratings
#| echo: true
print(confint.merMod(generalized_model, method='Wald'))
```


<br>
<br>
<h3> <b> Sensitivity analysis </b></h3>
We also ran the same analyses after outliers had been excluded, to assess whether outliers are driving this effect.

<p>The originally planned outlier criteria is not fit for purpose due to the large skew of the regressive error outcome. Instead, we run a sensitivity analysis excluding data-points that are outliers in terms of accuracy</p>

<p>Exclude outliers according to this definition
```{python}
#| code-fold: true
Q1 = task_summary["percentage_correct"].quantile(0.25)
Q3 = task_summary["percentage_correct"].quantile(0.75)

IQR_value = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR_value
upper_bound = Q3 + 1.5 * IQR_value

explore_df = task_summary[task_summary["percentage_correct"] >= lower_bound]

```

```{python}
#| label: Skewness alt outlier
#| echo: true
#| code-fold: true
pt=PowerTransformer(method='yeo-johnson', standardize=False)
skl_yeojohnson=pt.fit(pd.DataFrame(explore_df.mean_regressive_er))
skl_yeojohnson=pt.transform(pd.DataFrame(explore_df.mean_regressive_er))
explore_df['regressive_er_transformed'] = pt.transform(pd.DataFrame(explore_df.mean_regressive_er))

fig, axes = plt.subplots(1, 2, sharey=True)
sns.histplot(data=explore_df, x="mean_regressive_er", ax=axes[0]) 
sns.histplot(data=explore_df['regressive_er_transformed'], ax=axes[1])
print('Regressive error skew: '+str(skew(explore_df.mean_regressive_er)))
```

<p>Mixed effects models assumptions are violated</p>
In this case, the basic model (no random slopes or random intercepts, and no covariates) produced the best fit (indexed by BIC scores).
<p>But the model assumptions were violated</p>
```{python}
#| label: Mixed effects model - model selection alt outlier
#| echo: true
#| code-fold: true
data=explore_df.reset_index()

formula = 'regressive_er_transformed ~ block_type'

basic_model=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)

#test which random effects to include
#feedback_randint=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'feedback_details': '0+feedback_details'}).fit(reml=False) CONVERGENCE WARNING
#fractals_randint=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'fractals': '0+fractals'}).fit(reml=False) CONVERGENCE WARNING
feedback_fractals_randint=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={"feedback_details": "0 + feedback_details", "fractals": "0 + fractals"}).fit(reml=False)

randslope=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', re_formula='~block_type').fit(reml=False)
feedback_randint_randslope=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'feedback_details': '0+feedback_details'}, re_formula='~block_type').fit(reml=False)
#feedback_fractals_randint_randslope=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'feedback_details': '0+feedback_details', "fractals": "0 + fractals"}, re_formula='~block_type').fit(reml=False) CONVERGENCE WARNING


bic=pd.DataFrame({'basic_model': [basic_model.bic], 
                   # 'feedback_andint': ['CONVERGENCE WARNING'], 
                  #  'fractals_randint': ['CONVERGENCE WARNING'],
                    'feedback_fractals_randint': [feedback_fractals_randint.bic],
                    'randslope': [randslope.bic],
                    'feedback_randint_randslope':[feedback_randint_randslope.bic],
                   # 'feedback_fractals_randint_randslope': ['CONVERGENCE WARNING']
                   })
win1=bic.sort_values(by=0, axis=1).columns[0]

##test which covariates to add -- Using the random effects which were best above (basic model in this case)
no_covariate=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_covariate=smf.mixedlm(formula+str('+prolific_sex'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
age_covariate=smf.mixedlm(formula+str('+prolific_age'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
digit_span_covariate=smf.mixedlm(formula+str('+digit_span'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_age_covariate=smf.mixedlm(formula+str('+prolific_sex+prolific_age'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_digit_span_covariate=smf.mixedlm(formula+str('+prolific_sex+digit_span'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
digit_span_age_covariate=smf.mixedlm(formula+str('+digit_span+prolific_age'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_age_digit_span_covariate=smf.mixedlm(formula+str('+prolific_sex+prolific_age+digit_span'), data, groups=data['participant_no'], missing='drop').fit(reml=False)

bic=pd.DataFrame({'no_covariate': [no_covariate.bic], 
                    'sex_covariate': [sex_covariate.bic], 
                    'age_covariate': [age_covariate.bic],
                    'digit_span_covariate': [digit_span_covariate.bic],
                    'sex_age_covariate': [sex_age_covariate.bic],
                    'sex_digit_span_covariate': [sex_digit_span_covariate.bic],
                    'digit_span_age_covariate': [digit_span_age_covariate.bic],
                    'sex_age_digit_span_covariate': [sex_age_digit_span_covariate.bic]})
win2=bic.sort_values(by=0, axis=1).columns[0]
print("Winning models: "+ win1 +" "+ win2)
```

<p>Shapiro-Wilk test of normality of residuals (violated)</p>
```{python}
#| label: Mixed effects model -shapiro-wilk alt outlier
#| echo: true
#| code-fold: true
#chosen model
results=no_covariate

#shapiro-Wilk test of normality of residuals
labels = ["Statistic", "p-value"]
norm_res = stats.shapiro(results.resid)

for key, val in dict(zip(labels, norm_res)).items():
    print(key, val)
    ##if test is significant then the assumption is violated
        #is significant here
```

<p> White Lagrange multiplier Test for Heteroscedasticity (not violated) </p>
```{python}
#| label: Mixed effects model -white lagrange alt outlier
#| echo: true
#| code-fold: true
#chosen model
##homoskedasticity of variance 
#White Lagrange Multiplier Test for Heteroscedasticity
het_white_res = het_white(results.resid, results.model.exog)

labels = ["LM Statistic", "LM-Test p-value", "F-Statistic", "F-Test p-value"]

for key, val in dict(zip(labels, het_white_res)).items():
    print(key, val)
    ##again, only violated if you get a significant p value
```

<h4>So we run a generalized mixed effects model (done in R)</h4>

```{R}
#| label: alternative outlier exclusion R
#| code-fold: true
task_summary <- read.csv("U:/Documents/Disgust learning project/github/disgust_reversal_learning-final/csvs/dem_vids_task_excluded.csv")
Q1 <- quantile(task_summary$percentage_correct, 0.25)
Q3 <- quantile(task_summary$percentage_correct, 0.75)

IQR_value <- Q3 - Q1  

lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value

explore_df <- task_summary[task_summary$percentage_correct >= lower_bound, ]
```

Model details:
<p>
* Gamma probability distribution and inverse link function
* no additional random effects
* no additional covariates
</p>
<p>This is the specification that produced the best fit (according to BIC)</p>

```{r}
#| label: generalized mixed effects model selection alt outlier
#| echo: true
#| code-fold: true
#| warning: false

explore_df$pos_regressive_er <- explore_df$mean_regressive_er + 0.01 ##+0.01 as all values must be positive (i.e., can't have 0s)

gamma_log <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=explore_df, family=Gamma(link="log"))
gamma_inverse <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=explore_df, family=Gamma(link="inverse"))
#gamma_identity <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=explore_df, family=Gamma(link="identity"))

invgaus_log <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=explore_df, family=inverse.gaussian(link="log"))
invgaus_inverse <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=explore_df, family=inverse.gaussian(link="inverse"))
#invgaus_identity <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=explore_df, family=inverse.gaussian(link="identity"))

bic_values <- c(
  BIC(gamma_inverse),
  BIC(invgaus_log),
  BIC(invgaus_inverse)
)
model_names <- c("Gamma (inverse)", "inverse gaussian (log)", "inverse gaussian (inverse)")

bic_df <- data.frame(Model = model_names, BIC = bic_values)
win1 <- bic_df[which.min(bic_df$BIC), ]$Model
win1 <- bic_df[which.min(bic_df$BIC), ]$Model


basic_model <- glmer(pos_regressive_er ~ block_type + (1|participant_no), data=explore_df, family=Gamma(link="inverse"))

feedback_randint <- glmer(pos_regressive_er ~ block_type + (1|participant_no) + (1|feedback_details), data=explore_df, family=Gamma(link="inverse"))
#fractals_randint <- glmer(pos_regressive_er ~ block_type + (1|participant_no) + (1|fractals), data=explore_df, family=Gamma(link="inverse"))
#feedback_fractals_randint <- glmer(pos_regressive_er ~ block_type + (1|participant_no) + (1|fractals) + (1|feedback_details), data=explore_df, family=Gamma(link="inverse"))

#randslope <- glmer(pos_regressive_er ~ block_type + (block_type|participant_no), data=explore_df, family=Gamma(link="inverse"))
#feedback_randint_randslope <- glmer(pos_regressive_er ~ block_type + (block_type|participant_no) + (1|feedback_details), data=explore_df, family=Gamma(link="inverse"))
#feedback_fractals_randint_randslope <- glmer(pos_regressive_er ~ block_type + (block_type|participant_no) + (1|feedback_details) + (1|fractals), data=explore_df, family=Gamma(link="inverse"))

bic_values <- c(
  BIC(basic_model),
  BIC(feedback_randint)
)
model_names <- c("basic model", "feedback_randint")

bic_df <- data.frame(Model = model_names, BIC = bic_values)
win2 <- bic_df[which.min(bic_df$BIC), ]$Model

no_covariate <- basic_model

sex_covariate <- glmer(pos_regressive_er ~ block_type + (1|participant_no) + prolific_sex, data=explore_df, family=Gamma(link="inverse"))
#age_covariate <- glmer(pos_regressive_er ~ block_type + (1|participant_no) + prolific_age, data=explore_df, family=Gamma(link="inverse"))
digit_span_covariate <- glmer(pos_regressive_er ~ block_type + (1|participant_no) + digit_span, data=explore_df, family=Gamma(link="inverse"))
#sex_age_covariate <- glmer(pos_regressive_er ~ block_type + (1|participant_no) + prolific_sex + prolific_age, data=explore_df, family=Gamma(link="inverse"))
#sex_digit_span_covariate <- glmer(pos_regressive_er ~ block_type + (1|participant_no) + prolific_sex + digit_span, data=explore_df, family=Gamma(link="inverse"))
#digit_span_age_covariate <- glmer(pos_regressive_er ~ block_type + (1|participant_no) + digit_span + prolific_age, data=explore_df, family=Gamma(link="inverse"))
#sex_digit_span_age_covariate <- glmer(pos_regressive_er ~ block_type + (1|participant_no) + prolific_sex + prolific_age + digit_span, data=explore_df, family=Gamma(link="inverse"))

bic_values <- c(
  BIC(no_covariate),
  BIC(digit_span_covariate)
)
model_names <- c("no_covariate", "digit_span_covariate")

bic_df <- data.frame(Model = model_names, BIC = bic_values)
win3 <- bic_df[which.min(bic_df$BIC), ]$Model

print(paste0("Winning models: ", win1, " ", win2," ",win3))
```


<p>Results from this model show <b>no effect of block-type</b> on regressive error rate.</p>
```{r}
#| label: generalized mixed effects model alt outlier
#| code-fold: true
#| echo: true
generalized_model <- no_covariate
summary(generalized_model)
```

<br>
<p>As this hypothesis test found a no difference between fear and disgust or disgust and points, we will compute a Bayes Factor to test the strength of the evidence for the null</p>

<p>Firstly for disgust vs fear:</p>
```{python}
#| label: disgust vs fear bayes factor sensitivity alt outlier
#| echo: true
#| code-fold: true

ttest, bf_null = bayes_factor(explore_df, 'mean_regressive_er', 'Disgust', 'Fear')

print(f"Disgust vs Fear: BF01 = {bf_null}")
```
<br>
<p>Next for disgust vs points:</p>
```{python}
#| label: disgust vs points bayes factor - sensitivity alt outlier
#| echo: true
#| code-fold: true

ttest, bf_null = bayes_factor(explore_df, 'mean_regressive_er', 'Disgust', 'Points')
#print("Disgust vs Fear BF01: " + bf_null)

print(f"Disgust vs Points: BF01 = {bf_null}")
```

<br>
<p>We also look at fear vs points (which is not directly assessed by the model)</p>
```{python}
#| label: points vs fear ttest - sensitivity alt outlier
#| echo: true
#| code-fold: true
ttest, bf_null = bayes_factor(explore_df, 'mean_regressive_er', 'Points', 'Fear')

print(f"Points vs Fear: T = {ttest['T'][0]}, CI95% = {ttest['CI95%'][0]}, p = {ttest['p-val'][0]}")
```

<p>And because the result is null, also get a Bayes factor: </p>
```{python}
#| label: points vs fear bayes factor - sensitivity alt outlier
#| echo: true
#| code-fold: true
print(f"Points vs Fear: BF01 = {bf_null}")
```


<h3>Pre-registered sensitivity analysis</h3>
<p>The planned sensitivity analysis is included for completeness</p>
<p>Firstly, exclude outliers from the dataframe (outliers are define as those >1.5 IQRs above or below the upper or lower quartile)

```{python}
#| label: exclude outliers
#| echo: true
#| code-fold: true
#create outliers df --> removing those >1.5 IQRs above or below UQ and LQ
def replace_outliers_with_nan(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1- 1.5 *  IQR
    upper_bound = Q3 + 1.5 *  IQR
    df[column]=df[column].apply(lambda x: np.nan if x<lower_bound or x>upper_bound else x)
    return df

key_outcomes=['percentage_correct', 'mean_perseverative_er', 'mean_regressive_er', 'median_till_correct', 'win_stay', 'lose_shift']
for col in key_outcomes:
    task_summary=replace_outliers_with_nan(task_summary, col)
task_summary.to_csv('sensitivity_df.csv')
sensitivity_df=task_summary
```

<br>
<h3>Assess and correct for skewness in perservative error outcome (excluding outliers)</h3>
```{python}
#| label: Skewness sensitivity
#| echo: true
#| code-fold: true
pt=PowerTransformer(method='yeo-johnson', standardize=False)
skl_yeojohnson=pt.fit(pd.DataFrame(sensitivity_df.mean_regressive_er))
skl_yeojohnson=pt.transform(pd.DataFrame(sensitivity_df.mean_regressive_er))
sensitivity_df['regressive_er_transformed'] = pt.transform(pd.DataFrame(sensitivity_df.mean_regressive_er))


fig, axes = plt.subplots(1,2, sharey=True)
sns.histplot(data=sensitivity_df, x="mean_regressive_er", ax=axes[0]) 
sns.histplot(data=sensitivity_df['regressive_er_transformed'], ax=axes[1])
print('regressive error skew: '+str(skew(sensitivity_df.mean_regressive_er.dropna())))
```

<h3><b>Outlier-free hypothesis testing</b></h3>
In this case, the basic model (no random slopes or random intercepts) with no additional covariates produced the best fit (indexed by BIC scores).

```{python}
#| label: Mixed effects model sensitivity - model selection
#| echo: true
#| code-fold: true

data=task_summary.reset_index()

formula = 'regressive_er_transformed ~ block_type'

basic_model=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)

#test which random effects to include
#feedback_randint=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'feedback_details': '0+feedback_details'}).fit(reml=False)
#fractals_randint=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'fractals': '0+fractals'}).fit(reml=False)
#feedback_fractals_randint=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={"feedback_details": "0 + feedback_details", "fractals": "0 + fractals"}).fit(reml=False)
        #had to comment out because it does not converge and errors out

randslope=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', re_formula='~block_type').fit(reml=False)
feedback_randint_randslope=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'feedback_details': '0+feedback_details'}, re_formula='~block_type').fit(reml=False)
#feedback_fractals_randint_randslope=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop', vc_formula={'feedback_details': '0+feedback_details', "fractals": "0 + fractals"}, re_formula='~block_type').fit(reml=False)
       

bic=pd.DataFrame({'basic_model': [basic_model.bic], 
                   # 'feedback_andint': ['CONVERGENCE WARNING'], 
                   # 'fractals_randint': ['CONVERGENCE WARNING'],
                   # 'feedback_fractals_randint': ['NOT CONVERGED'],
                    'randslope': [randslope.bic],
                    'feedback_randint_randslope':[feedback_randint_randslope.bic],
                    #'feedback_fractals_randint_randslope': ['NOT CONVERGED']
                    })
win1=bic.sort_values(by=0, axis=1).columns[0]

no_covariate=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_covariate=smf.mixedlm(formula+str('+prolific_sex'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
age_covariate=smf.mixedlm(formula+str('+prolific_age'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
digit_span_covariate=smf.mixedlm(formula+str('+digit_span'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_age_covariate=smf.mixedlm(formula+str('+prolific_sex+prolific_age'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_digit_span_covariate=smf.mixedlm(formula+str('+prolific_sex+digit_span'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
digit_span_age_covariate=smf.mixedlm(formula+str('+digit_span+prolific_age'), data, groups=data['participant_no'], missing='drop').fit(reml=False)
sex_age_digit_span_covariate=smf.mixedlm(formula+str('+prolific_sex+prolific_age+digit_span'), data, groups=data['participant_no'], missing='drop').fit(reml=False)

bic=pd.DataFrame({'no_covariate': [no_covariate.bic], 
                    'sex_covariate': [sex_covariate.bic], 
                    'age_covariate': [age_covariate.bic],
                    'digit_span_covariate': [digit_span_covariate.bic],
                    'sex_age_covariate': [sex_age_covariate.bic],
                    'sex_digit_span_covariate': [sex_digit_span_covariate.bic],
                    'digit_span_age_covariate': [digit_span_age_covariate.bic],
                    'sex_age_digit_span_covariate': [sex_age_digit_span_covariate.bic]})
win2=bic.sort_values(by=0, axis=1).columns[0]
print("Winning models: "+ win1 +" "+ win2)
```

<h4>This time the assumptions are not violated - so no generalized model needs to be run. </h4>
<p>Shapiro-Wilk test of normality of residuals</p>
```{python}
#| label: Mixed effects model sensitivity-shapiro-wilk
#| echo: true
#| code-fold: true
#chosen model
results=no_covariate

#shapiro-Wilk test of normality of residuals
labels = ["Statistic", "p-value"]
norm_res = stats.shapiro(results.resid)

for key, val in dict(zip(labels, norm_res)).items():
    print(key, val)
    ##if test is significant then the assumption is violated
        #is significant here
```

<p> White Lagrange multiplier Test for Heteroscedasticity </p>
```{python}
#| label: Mixed effects model sensitivity -white lagrange
#| echo: true
#| code-fold: true
#chosen model
##homoskedasticity of variance 
#White Lagrange Multiplier Test for Heteroscedasticity
het_white_res = het_white(results.resid, results.model.exog)

labels = ["LM Statistic", "LM-Test p-value", "F-Statistic", "F-Test p-value"]

for key, val in dict(zip(labels, het_white_res)).items():
    print(key, val)
    ##again, only violated if you get a significant p value
```

<h4> Hypothesis test </h4>
```{python}
#| label: Mixed effects model - sensitivity
#| echo: true
data=sensitivity_df.reset_index()
formula = 'regressive_er_transformed ~ block_type'

results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<br>
<p>As this hypothesis test found a no difference between fear and disgust or disgust and points, we will compute a Bayes Factor to test the strength of the evidence for the null</p>

<p>Firstly for disgust vs fear:</p>
```{python}
#| label: disgust vs fear bayes factor sensitivity
#| echo: true
#| code-fold: true

ttest, bf_null = bayes_factor(sensitivity_df, 'mean_regressive_er', 'Disgust', 'Fear')

print(f"Disgust vs Fear: BF01 = {bf_null}")
```
<br>
<p>Next for disgust vs points:</p>
```{python}
#| label: disgust vs points bayes factor - sensitivity
#| echo: true
#| code-fold: true

ttest, bf_null = bayes_factor(sensitivity_df, 'mean_regressive_er', 'Disgust', 'Points')
#print("Disgust vs Fear BF01: " + bf_null)

print(f"Disgust vs Points: BF01 = {bf_null}")
```

<br>
<p>We also look at fear vs points (which is not directly assessed by the model)</p>
```{python}
#| label: points vs fear ttest - sensitivity
#| echo: true
#| code-fold: true
ttest, bf_null = bayes_factor(sensitivity_df, 'mean_regressive_er', 'Points', 'Fear')

print(f"Points vs Fear: T = {ttest['T'][0]}, CI95% = {ttest['CI95%'][0]}, p = {ttest['p-val'][0]}")
```

<p>And because the result is null, also get a Bayes factor: </p>
```{python}
#| label: points vs fear bayes factor - sensitivity
#| echo: true
#| code-fold: true
print(f"Points vs Fear: BF01 = {bf_null}")
```
