---
title: "Model 4: lose shift probability ~ feedback type"
format: gfm
fig-format: jpeg
---

<p>This file contains all model-agnostic tests run to test the effect of feedback type (fear, disgust, points) on lose-shift probability.</p>
<br>
Includes:
<p>
* initial skew assessment
* initial hypothesis testing mixed effects model
* assessment of assumptions of this model
* assessing whether adding video-ratings differences (identified in video-rating analyses) moderates results
* sensitivity analysis 
* final conclusions
</p>
<h3>Load in packages and data- in python and in r </h3>
```{python}
#| label: Python packages
#| echo: true
#| code-fold: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import jsonlines
from functools import reduce
import statistics
import scipy.stats
import seaborn as sns
import math
import os
import json
import ast
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pingouin as pg
import warnings
from scipy.stats import ttest_rel
#from statannotations.Annotator import Annotator
from scipy.stats import skew
from statsmodels.stats.diagnostic import het_white
from sklearn.preprocessing import PowerTransformer
import statannot
from scipy.stats import ttest_ind
import itertools

warnings.simplefilter(action='ignore', category=FutureWarning)
pd.options.mode.copy_on_write = True

task_summary=pd.read_csv("U:/Documents/Disgust learning project/github/disgust_reversal_learning-final/csvs/dem_vids_task_excluded.csv")
```

```{r, message=FALSE}
#| label: R packages
#| echo: true
#| code-fold: true
library(tidyverse, quietly=TRUE)
library(lme4)
library(emmeans)
library(DHARMa)

task_summary <- read.csv("U:/Documents/Disgust learning project/github/disgust_reversal_learning-final/csvs/dem_vids_task_excluded.csv")
```

<br>
<h3><b>Visualise the data</b></h3>
```{python}
#| label: Visualisation
#| echo: true
#| code-fold: true

palette = ["#F72585", "#3A0CA3", "#4CC9F0"]

##plot hypothesised results
fig, axes = plt.subplots(1,1, sharey=False)

sns.stripplot(data=task_summary, x="block_type", y="lose_shift", ax=axes, palette=palette, size=5, jitter=True, marker='.')
sns.violinplot(data=task_summary, x="block_type", y="lose_shift", ax=axes,fill=True, inner="quart", palette=palette, saturation=0.5, cut=0)
#axes.set_xlabel("Feedback type")
axes.set_xlabel("")
axes.set_xticklabels(axes.get_xticklabels(), rotation=0)
axes.set_ylabel("Lose-shift probability") 
axes.set_title("Lose-shift")
```


<br>
<h3>The lose-shift variable is not skewed</h3>
<p>So no skewness transformation is required.</p>
```{python}
#| label: Skewness
#| echo: true
#| code-fold: true
sns.histplot(data=task_summary, x="lose_shift") 
print('lose-shift  skew: '+str(skew(task_summary.lose_shift, nan_policy='omit')))
```

<h3><b>Hypothesis testing</b></h3>
<p>In this case, the basic model (no random slopes or random intercepts)  with an age covariate produced the best fit (indexed by BIC scores).
<p>This model showed a <b>significant effect of feedback-type on lose-shift probability</b></p>
<p>Specifically, the points block had a significantly higher lose-shift probability than the disgust-block.</p>
<p>There is also a significant effect of age on lose-shift probability.</p>
```{python}
#| label: Mixed effects model
#| echo: true
data=task_summary
formula = 'lose_shift ~ block_type + prolific_age'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<p>The assumptions for this model are not violated</p>
```{python}
#| label: Shapiro-Wilk test
#| echo: true

#shapiro-Wilk test of normality of residuals
labels = ["Statistic", "p-value"]
norm_res = stats.shapiro(results.resid)
for key, val in dict(zip(labels, norm_res)).items():
    print(key, val)
```

```{python}
#| label: Homoskedasticity test 
#| echo: true
##homoskedasticity of variance 
#White Lagrange Multiplier Test for Heteroscedasticity
het_white_res = het_white(results.resid, results.model.exog)

labels = ["LM Statistic", "LM-Test p-value", "F-Statistic", "F-Test p-value"]

for key, val in dict(zip(labels, het_white_res)).items():
    print(key, val)
```

<p>And the results remain unchanged when the age covariate is dropped:</p>
```{python}
#| label: Mixed effects model - no covariate
#| echo: true
formula = 'lose_shift ~ block_type'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<br>
<p>As this hypothesis test found a no difference between fear and disgust, we will compute a Bayes Factor to test the strength of the evidence for the null</p>
```{python}
#| label: bayes factor function
#| echo: true
#| code-fold: true
def bayes_factor(df, dependent_var, condition_1_name, condition_2_name):
    df=df[(df.block_type==condition_1_name)| (df.block_type==condition_2_name)][[dependent_var, 'block_type', 'participant_no']]
    df.dropna(inplace=True)
    df=df.pivot(index='participant_no', columns='block_type', values=dependent_var).reset_index()
    ttest=pg.ttest(df[condition_1_name], df[condition_2_name], paired=True)
    bf_null=1/float(ttest.BF10)
    return bf_null
```

```{python}
#| label: disgust vs fear bayes factor
#| echo: true
print(bayes_factor(task_summary, 'lose_shift', 'Disgust', 'Fear'))
```
<p>This means that there is <b>moderate</b> evidence for the null</p>

<p>We also look at fear vs points (which is not directly assessed by the model)</p>
```{python}
#| label: points vs fear bayes factor
#| echo: true
print(bayes_factor(task_summary, 'lose_shift', 'Points', 'Fear'))
```
<p>This means that there is <b>weak</b> evidence for the null</p>


<br>
<p><b>Next, we showed that this result is unchanged by the addition of video-rating covariates.</b></p>
<p>I.e., there is a main effect of block-type (driven by a difference between disgust and points) and a main effect of age
<p>(again, the model with no additional random effects/slopes, with an age covariate produced the best fit)</p>
```{python}
#| label: mixed effects model with video ratinsg
#| echo: true
formula = 'lose_shift ~ block_type + valence_diff + arousal_diff + valence_habdiff + prolific_age'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<p>However, this model fails the homoskedasticity of variance test</p>
```{python}
#| label: Homoskedasticity test -video ratings
#| echo: true
##homoskedasticity of variance 
#White Lagrange Multiplier Test for Heteroscedasticity
het_white_res = het_white(results.resid, results.model.exog)

labels = ["LM Statistic", "LM-Test p-value", "F-Statistic", "F-Test p-value"]

for key, val in dict(zip(labels, het_white_res)).items():
    print(key, val)
```

<p>So a generalized mixed effects model needs to be run (using R)</p>
<p>Model details:
<p>
* Gamma probability distribution and identity link function
* no additional random intercepts or by-participant random slopes
* no additional covariates
</p>
<p>This is the specification that produced the best fit (according to BIC)</p>
```{r}
#| label: generalized mixed effects model
#| echo: true
generalized_model <- glmer(lose_shift ~ block_type + valence_diff + arousal_diff + valence_habdiff + (1|participant_no), data=task_summary, family=Gamma(link="identity"))
summary(generalized_model)
```

<br>
<br>
<h3> <b> Sensitivity analysis </b></h3>
<p>No sensitivity analysis is required for this outcome variable, because no values meet the criteria of being an outlier (>1.5 IQR below LQ or above UQ)</p>

```{python}
#| label: exclude outliers
#| echo: true
#| code-fold: true
#create outliers df --> removing those >1.5 IQRs above or below UQ and LQ
def replace_outliers_with_nan(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1- 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[column]=df[column].apply(lambda x: np.nan if x<lower_bound or x>upper_bound else x)
    return df

key_outcomes=['percentage_correct', 'mean_perseverative_er', 'mean_regressive_er', 'median_till_correct', 'win_stay', 'lose_shift']
for col in key_outcomes:
    task_summary=replace_outliers_with_nan(task_summary, col)
task_summary.to_csv('sensitivity_df.csv')
sensitivity_df=task_summary

print("Number of lose-shift outliers: "+str(len(sensitivity_df[sensitivity_df.lose_shift.isna()])))
```


