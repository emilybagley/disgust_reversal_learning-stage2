---
title: "Exploratory analyses"
format: gfm
fig-format: jpeg
---

<p>This file contains all of the exploratory analyses run - these are analyses not specified in the stage 1 registered report, but are run to help understand our model-agnostic results better</p>

<br>
<p>This includes assessing whether:</p>
<p>
* Overall task performance (index by percentage of correct trials) differs across the three blocks/feedback types
* The difference between self-reported points and disgust ratings explains the difference between points and disgust learning
* there is anything noteable about the outliers on the perseverative error outcome - explaining why they drive effects
* Video ratings for *all* videos (not just the ones used in the reversal learning task) show similar patterns to those selected for use in the reversal learning task.
</p>

<h3>Firstly, we load in the relevant packages for both R and python:</h3>

```{r, message=FALSE}
#| label: R packages
#| echo: true
#| code-fold: true
library(tidyverse, quietly=TRUE)
library(lme4)
library(emmeans)
library(DHARMa)

task_summary <- read.csv("U:/Documents/Disgust learning project/github/disgust_reversal_learning-final/csvs/dem_vids_task_excluded.csv")
chosen_stim_df <- read.csv('U:/Documents/Disgust learning project/github/disgust_reversal_learning-final/csvs/chosen_stim_excluded.csv')
sensitivity_df <- read.csv('U:/Documents/Disgust learning project/github/disgust_reversal_learning-final/csvs/sensitivity_df.csv')
```

```{python}
#| label: Python packages
#| echo: true
#| code-fold: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import jsonlines
from functools import reduce
import statistics
import scipy.stats
import seaborn as sns
import math
import os
import json
import ast
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pingouin as pg
import warnings
from scipy.stats import ttest_rel
#from statannotations.Annotator import Annotator
from scipy.stats import skew
from statsmodels.stats.diagnostic import het_white
from sklearn.preprocessing import PowerTransformer
import statannot
from scipy.stats import ttest_ind
import itertools

warnings.simplefilter(action='ignore', category=FutureWarning)
pd.options.mode.copy_on_write = True

task_summary=pd.read_csv("U:/Documents/Disgust learning project/github/disgust_reversal_learning-final/csvs/dem_vids_task_excluded.csv")
chosen_stim_df=pd.read_csv('U:/Documents/Disgust learning project/github/disgust_reversal_learning-final/csvs/chosen_stim_excluded.csv')
sensitivity_df = pd.read_csv('U:/Documents/Disgust learning project/github/disgust_reversal_learning-final/csvs/sensitivity_df.csv')

def bayes_factor(df, dependent_var, condition_1_name, condition_2_name):
    df=df[(df.block_type==condition_1_name)| (df.block_type==condition_2_name)][[dependent_var, 'block_type', 'participant_no']]
    df.dropna(inplace=True)
    df=df.pivot(index='participant_no', columns='block_type', values=dependent_var).reset_index()
    ttest=pg.ttest(df[condition_1_name], df[condition_2_name], paired=True)
    bf_null=1/float(ttest.BF10)
    return bf_null
```

<h3><b>Exploratory analysis 1: </b> </h3>
<p>Hypothesis testing analyses showed a difference in perseverative error rate and lose-shift probability between fear and points learning. To better understand this change, we tested whether this difference is mirrored by a difference in overall task performance (indexed by percentage of trials where participants were correct)</p>
<br>
<p>Firstly, we can plot this variable:</p>
```{python}
#| label: Percentage correct
#| echo: true
#| code-fold: true
palette = ["#F72585", "#3A0CA3", "#4CC9F0"]

##plot hypothesised results
fig, axes = plt.subplots(1,1, sharey=False)

sns.stripplot(data=task_summary, x="block_type", y="percentage_correct", ax=axes, palette=palette, size=5, jitter=True, marker='.')
sns.violinplot(data=task_summary, x="block_type", y="percentage_correct", ax=axes,fill=True, inner="quart", palette=palette, saturation=0.5)
#axes.set_xlabel("Feedback type")
axes.set_xlabel("")
axes.set_xticklabels(axes.get_xticklabels(), rotation=0)
axes.set_ylabel("Percentage correct") 
axes.set_title("Percentage correct")
```

<br>
<p>We also check for skewness of the variable</p>
```{python}
#| label: Skewness percentage correct

sns.histplot(data=task_summary, x="percentage_correct") 
print('Percentage correct: '+str(skew(task_summary.percentage_correct)))

```

<br>
<p>And then run a mixed effects model to assess whether it differs by block-type</p>
<p>The regular mixed effects model did not converge, so a generalized mixed effects model is run in its place</p>
<p>The winning model (as indexed by BIC) had:
<p>
* a gamma probability function
* an identity link function
* no covariates
* random intercepts for feedback videos
```{r}
#| label: generalized mixed effects model - percentage correct

data<-task_summary

generalized_model <- glmer(percentage_correct ~ block_type +(1|participant_no) + (1|feedback_details), data=data, family=Gamma(link="identity"))
summary(generalized_model)
```

<p>The results of the outlier-freesensitivity analysis model is similar (although it is closer to significance)</p>
```{r}
#| label: generalized mixed effects model - percentage correct sensitivity

data<-sensitivity_df

generalized_model <- glmer(percentage_correct ~ block_type +(1|participant_no), data=data, family=Gamma(link="identity"))
summary(generalized_model)
```

<p>Bayes factors also show support for the null hypothesis (no difference between points and disgust):</p>
<b>With outliers:</b>
```{python}
#| label: percentage correct BF
#| echo: true
print(bayes_factor(task_summary, 'percentage_correct', 'Disgust', 'Points'))
```
<p>This means that there is <b>moderate</b> evidence for the null</p>
<br>
<b>Without outliers</b>:
```{python}
#| label: percentage correct BF - sensitivity
#| echo: true
print(bayes_factor(sensitivity_df, 'percentage_correct', 'Disgust', 'Points'))
```
<p>This means that there is <b>moderate</b> evidence for the null</p>

<br>
<br>
<h3><b>Exploratory analysis 2: </b> assessing the effect of video ratings on lose-shift results</h3>
<p>In the planned analysis, we assessed whether all hypothesis testing models were affected by differences between <b>fear and disgust</b> detected in the video rating analyses</p>
<p>These analyses were planned in order to assess whether any differences between fear and disgust were driven by any differences in valence and arousal</p>
<p>However, since no hypothesis testing model found a difference between fear and disgust, instead finding a difference bewteen <b>points and disgust</b> - arguably these video ratings were not the most relevant ones to include.</p>
<p>Instead, it makes more sense to test whether the difference between <b>points and disgust</b> is driven by the differences between <b>points and disgust</b> found in the video rating analyses.</p>
<p>Namely, whether the difference between disgust and points learning (indexed by lose-shift probability) is driven by:
<p>
* the difference in valence found between points and disgust feedback
* the difference in fear rating found between points and disgust
* the difference in disgust rating found between points and disgust
</p>
<p>NB lose-shift is chosen for this analysis as it is the most robust of our findings (relative to the perseverative error finding) so allows more firm conclusions

<b>Due to an error in the task code, we don't have points-ratings values for some participants. Given that these analyses pertain to points-ratings, we will first exclude all these participants.</p>
```{python}
#| label: remove participants with no points ratings
participants_to_remove=list(set(chosen_stim_df[chosen_stim_df.unpleasant_1.isna()].participant_no))
chosen_stim_df_short=chosen_stim_df[~chosen_stim_df['participant_no'].isin(participants_to_remove)]
task_summary_short=task_summary[~task_summary['participant_no'].isin(participants_to_remove)]
```

<p>NB we ran a sanity check to show that the original 'lose-shift' result remains in this slightly smaller sample</p>
```{python}
#| label: lose-shift sanity check
formula = 'lose_shift ~ block_type + prolific_age'
data=task_summary_short
results=smf.mixedlm(formula, data=data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<p>Also create a data-frame with points vs disgust ratings difference scores (as had originally done for disgust vs fear)</p>
```{python}
#| label: make points rating df
stim_ratings_covariates=pd.DataFrame()
block_feedback=pd.DataFrame()
for participant_no in set(chosen_stim_df_short.participant_no):
    participant_df=chosen_stim_df_short[chosen_stim_df_short.participant_no==participant_no]
    disgust=participant_df[participant_df.trial_type=="disgust"]
    points=participant_df[participant_df.trial_type=="points"]
    valence_diff=int(points.unpleasant_1)-int(disgust.unpleasant_1)
    disgust_diff=int(points.disgusting_1)-int(disgust.disgusting_1)
    fear_diff=int(points.frightening_1)-int(disgust.frightening_1)

    
    row=pd.DataFrame({
        'participant_no': [participant_no],
        'points_valence_diff': [valence_diff],
        'points_disgust_diff': [disgust_diff],
        'points_fear_diff': [fear_diff]
    })
    stim_ratings_covariates=pd.concat([stim_ratings_covariates, row])
data=pd.merge(task_summary_short, stim_ratings_covariates, on='participant_no', how='outer')
```

<p>In a series of mixed effects models we showed that the lose-shift result was <b>not</b> moderated by differences in video ratings between disgust and points</p>
<b>Valence difference between points and disgust</b>
```{python}
#| label: lose-shift plus points valence diff
formula = 'lose_shift ~ block_type + prolific_age + points_valence_diff'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<b>Disgust difference between points and disgust</b>
```{python}
#| label: lose-shift plus points disgust diff
formula = 'lose_shift ~ block_type + prolific_age + points_disgust_diff'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<b>Fear difference between points and disgust</b>
```{python}
#| label: lose-shift plus points fear diff
formula = 'lose_shift ~ block_type + prolific_age + points_fear_diff'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<br>
<h3><b>Exploratory analysis 3:</b> testing whether self-reported valence, arousal, fear and disgust relates to performance on the reversal learning task</h3>
<p>