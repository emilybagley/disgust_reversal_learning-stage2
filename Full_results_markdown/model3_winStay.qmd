---
title: "Model 3: win stay probability ~ feedback type"
format: gfm
fig-format: jpeg
---

<p>This file contains all model-agnostic tests run to test the effect of feedback type (fear, disgust, points) on win-stay probability.</p>
<br>
Includes:
<p>
* initial skew assessment (and resulting skew transformation)
* initial hypothesis testing mixed effects model
* assessment of assumptions of this model
* assessing whether adding video-ratings differences (identified in video-rating analyses) moderates results
* sensitivity analysis
* final conclusions
</p>
<h3>Load in packages and data- in python </h3>
```{python}
#| label: Python packages
#| echo: true
#| code-fold: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import jsonlines
from functools import reduce
import statistics
import scipy.stats
import seaborn as sns
import math
import os
import json
import ast
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pingouin as pg
import warnings
from scipy.stats import ttest_rel
#from statannotations.Annotator import Annotator
from scipy.stats import skew
from statsmodels.stats.diagnostic import het_white
from sklearn.preprocessing import PowerTransformer
import statannot
from scipy.stats import ttest_ind
import itertools

warnings.simplefilter(action='ignore', category=FutureWarning)
pd.options.mode.copy_on_write = True

task_summary=pd.read_csv("U:/Documents/Disgust learning project/github/disgust_reversal_learning-final/csvs/dem_vids_task_excluded.csv")
```

<br>
<h3><b>Visualise the data</b></h3>
```{python}
#| label: Visualisation
#| echo: true
#| code-fold: true

palette = ["#F72585", "#3A0CA3", "#4CC9F0"]

##plot hypothesised results
fig, axes = plt.subplots(1,1, sharey=False)

sns.stripplot(data=task_summary, x="block_type", y="win_stay", ax=axes, palette=palette, size=5, jitter=True, marker='.')
sns.violinplot(data=task_summary, x="block_type", y="win_stay", ax=axes,fill=True, inner="quart", palette=palette, saturation=0.5, cut=0)
#axes.set_xlabel("Feedback type")
axes.set_xlabel("")
axes.set_xticklabels(axes.get_xticklabels(), rotation=0)
axes.set_ylabel("Win-stay probability") 
axes.set_title("Win-stay")
```
<br>

<br>
<h3>Assess and correct for skewness in win-stay outcome</h3>
```{python}
#| label: Skewness
#| echo: true
#| code-fold: true
pt=PowerTransformer(method='yeo-johnson', standardize=False)
skl_yeojohnson=pt.fit(pd.DataFrame(task_summary.win_stay))
skl_yeojohnson=pt.transform(pd.DataFrame(task_summary.win_stay))
task_summary['win_stay_transformed'] = pt.transform(pd.DataFrame(task_summary.win_stay))

fig, axes = plt.subplots(1,2, sharey=True)
sns.histplot(data=task_summary, x="win_stay", ax=axes[0]) 
sns.histplot(data=task_summary['win_stay_transformed'], ax=axes[1])
print('Win-stay skew: '+str(skew(task_summary.win_stay)))
```

<h3><b>Hypothesis testing</b></h3>
In this case, the basic model (no random slopes or random intercepts) with an age covariate produced the best fit (indexed by BIC scores).
<p>The model shows no effect of feedback type, and a significant effect of age.</p>

```{python}
#| label: Mixed effects model
#| echo: true
data=task_summary
formula = 'win_stay_transformed ~ block_type + prolific_age'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<p>And the model assumptions are not violated</p>
```{python}
#| label: Shapiro-Wilk test
#| echo: true

#shapiro-Wilk test of normality of residuals
labels = ["Statistic", "p-value"]
norm_res = stats.shapiro(results.resid)
for key, val in dict(zip(labels, norm_res)).items():
    print(key, val)
```

```{python}
#| label: Homoskedasticity test 
#| echo: true
##homoskedasticity of variance 
#White Lagrange Multiplier Test for Heteroscedasticity
het_white_res = het_white(results.resid, results.model.exog)

labels = ["LM Statistic", "LM-Test p-value", "F-Statistic", "F-Test p-value"]

for key, val in dict(zip(labels, het_white_res)).items():
    print(key, val)
```

<p>And the results remain unchanged when the age covariate is dropped:</p>
```{python}
#| label: Mixed effects model - no covariate
#| echo: true
formula = 'win_stay_transformed ~ block_type'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```


<br>
<p>As this hypothesis test found a no difference between fear and disgust or disgust and points, we will compute a Bayes Factor to test the strength of the evidence for the null</p>

```{python}
#| label: bayes factor function
#| echo: true
#| code-fold: true
def bayes_factor(df, dependent_var, condition_1_name, condition_2_name):
    df=df[(df.block_type==condition_1_name)| (df.block_type==condition_2_name)][[dependent_var, 'block_type', 'participant_no']]
    df.dropna(inplace=True)
    df=df.pivot(index='participant_no', columns='block_type', values=dependent_var).reset_index()
    ttest=pg.ttest(df[condition_1_name], df[condition_2_name], paired=True)
    bf_null=1/float(ttest.BF10)
    return bf_null
```


<p>Firstly for disgust vs fear:</p>
```{python}
#| label: disgust vs fear bayes factor
#| echo: true
print(bayes_factor(task_summary, 'win_stay_transformed', 'Disgust', 'Fear'))
```
<p>This means that there is <b>strong</b> evidence for the null</p>

<br>
<p>Next for disgust vs points:</p>
```{python}
#| label: disgust vs points bayes factor
#| echo: true
print(bayes_factor(task_summary, 'win_stay_transformed', 'Disgust', 'Points'))
```
<p>This means that there is <b>strong</b> evidence for the null</p>
<br>
<p>We also look at fear vs points (which is not directly assessed by the model)</p>
```{python}
#| label: points vs fear bayes factor 
#| echo: true
print(bayes_factor(task_summary, 'win_stay_transformed', 'Points', 'Fear'))
```
<p>This means that there is <b>strong</b> evidence for the null</p>

<br>
<br>
<p><b>Next, we showed that this result is unchanged by the addition of video-rating covariates.</b></p>
<p>(again, the model with no additional random effects/slopes, with an age covariate produced the best fit)</p>
```{python}
#| label: mixed effects model with video ratinsg
#| echo: true
formula = 'win_stay_transformed ~ block_type + valence_diff + arousal_diff + valence_habdiff + prolific_age'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<p>Model assumptions are not violated</p>
```{python}
#| label: Shapiro-Wilk test - video ratings
#| echo: true

#shapiro-Wilk test of normality of residuals
labels = ["Statistic", "p-value"]
norm_res = stats.shapiro(results.resid)
for key, val in dict(zip(labels, norm_res)).items():
    print(key, val)
```

```{python}
#| label: Homoskedasticity test -video ratings
#| echo: true
##homoskedasticity of variance 
#White Lagrange Multiplier Test for Heteroscedasticity
het_white_res = het_white(results.resid, results.model.exog)

labels = ["LM Statistic", "LM-Test p-value", "F-Statistic", "F-Test p-value"]

for key, val in dict(zip(labels, het_white_res)).items():
    print(key, val)
```

<p>And the results remain unchanged when the age covariate is dropped:</p>
```{python}
#| label: Mixed effects model w video ratings- no covariate
#| echo: true
formula = 'win_stay_transformed ~ block_type + valence_diff + arousal_diff + valence_habdiff'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<br>
<br>
<h3> <b> Sensitivity analysis </b></h3>
We also ran the same analyses after outliers had been excluded, to assess whether outliers are driving this effect.

<p>Firstly, exclude outliers from the dataframe (outliers are define as those >1.5 IQRs above or below the upper or lower quartile)

```{python}
#| label: exclude outliers
#| echo: true
#| code-fold: true
#create outliers df --> removing those >1.5 IQRs above or below UQ and LQ
def replace_outliers_with_nan(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1- 1.5 *  IQR
    upper_bound = Q3 + 1.5 *  IQR
    df[column]=df[column].apply(lambda x: np.nan if x<lower_bound or x>upper_bound else x)
    return df

key_outcomes=['percentage_correct', 'mean_perseverative_er', 'mean_regressive_er', 'median_till_correct', 'win_stay', 'lose_shift']
for col in key_outcomes:
    task_summary=replace_outliers_with_nan(task_summary, col)
task_summary.to_csv('sensitivity_df.csv')
sensitivity_df=task_summary
```
<br>
<h3>Assess and correct for skewness in win-stay outcome (excluding outliers)</h3>

```{python}
#| echo: true
#| code-fold: true
pt=PowerTransformer(method='yeo-johnson', standardize=False)
skl_yeojohnson=pt.fit(pd.DataFrame(sensitivity_df.win_stay))
skl_yeojohnson=pt.transform(pd.DataFrame(sensitivity_df.win_stay))
sensitivity_df['win_stay_transformed'] = pt.transform(pd.DataFrame(sensitivity_df.win_stay))

fig, axes = plt.subplots(1,2, sharey=True)
sns.histplot(data=sensitivity_df['win_stay'], ax=axes[0])
sns.histplot(data=sensitivity_df['win_stay_transformed'], ax=axes[1])
print('Win-stay skew: '+str(skew(sensitivity_df.win_stay.dropna())))

```


<h3><b>Outlier-free hypothesis testing</b></h3>
In this case, the basic model (no random slopes or random intercepts) with just the age covariate produced the best fit (indexed by BIC scores).
<p>The model shows no effect of feedback type but found an effect of age (as with the original analysis)</p>

```{python}
#| label: Mixed effects model - sensitivity
#| echo: true
data=sensitivity_df.reset_index()
formula = 'win_stay_transformed ~ block_type +prolific_age'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<p>The assumptions are not violated for this model</p>
```{python}
#| label: Shapiro-Wilk test- sensitivity
#| echo: true

#shapiro-Wilk test of normality of residuals
labels = ["Statistic", "p-value"]
norm_res = stats.shapiro(results.resid)
for key, val in dict(zip(labels, norm_res)).items():
    print(key, val)
```

```{python}
#| label: Homoskedasticity test -sensitivity
#| echo: true
##homoskedasticity of variance 
#White Lagrange Multiplier Test for Heteroscedasticity
het_white_res = het_white(results.resid, results.model.exog)

labels = ["LM Statistic", "LM-Test p-value", "F-Statistic", "F-Test p-value"]

for key, val in dict(zip(labels, het_white_res)).items():
    print(key, val)
```


<p>And the results are unchanged when the age covariate is dropped</p>
```{python}
#| label: Mixed effects model - sensitivity no covariate
#| echo: true
data=sensitivity_df.reset_index()
formula = 'win_stay_transformed ~ block_type'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<br>
<p>As this hypothesis test found a no difference between fear and disgust or disgust and points, we will compute a Bayes Factor to test the strength of the evidence for the null</p>

<p>Firstly for disgust vs fear:</p>
```{python}
#| label: disgust vs fear bayes factor sensitivity
#| echo: true
print(bayes_factor(sensitivity_df, 'win_stay_transformed', 'Disgust', 'Fear'))
```
<p>This means that there is <b>strong</b> evidence for the null</p>

<br>
<p>Next for disgust vs points:</p>
```{python}
#| label: disgust vs points bayes factor sensitivity
#| echo: true
print(bayes_factor(sensitivity_df, 'win_stay_transformed', 'Disgust', 'Points'))
```
<p>This means that there is <b>strong</b> evidence for the null</p>

<br>
<p>We also look at fear vs points (which is not directly assessed by the model)</p>
```{python}
#| label: points vs fear bayes factor sensitivity
#| echo: true
print(bayes_factor(sensitivity_df, 'win_stay_transformed', 'Points', 'Fear'))
```
<p>This means that there is <b>strong</b> evidence for the null</p>


<br>
<p><b>Finally, adding video rating values to this model has no effect: </b></p>
(NB model with the age covariate, again, produced the best fit)
```{python}
#| label: Mixed effects model w video ratings- sensitivity 
#| echo: true
data=sensitivity_df.reset_index()
formula = 'win_stay_transformed ~ block_type + valence_diff + arousal_diff + valence_habdiff + prolific_age'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

<p>The assumptions for this test are not violated:</p>
```{python}
#| label: Shapiro-Wilk test- sensitivity w video ratings
#| echo: true

#shapiro-Wilk test of normality of residuals
labels = ["Statistic", "p-value"]
norm_res = stats.shapiro(results.resid)
for key, val in dict(zip(labels, norm_res)).items():
    print(key, val)
```

```{python}
#| label: Homoskedasticity test -sensitivity w video ratings
#| echo: true
##homoskedasticity of variance 
#White Lagrange Multiplier Test for Heteroscedasticity
het_white_res = het_white(results.resid, results.model.exog)

labels = ["LM Statistic", "LM-Test p-value", "F-Statistic", "F-Test p-value"]

for key, val in dict(zip(labels, het_white_res)).items():
    print(key, val)
```


<p>And this remains unchanged when the age covariate is removed:</p>
```{python}
#| label: Mixed effects model w video ratings- sensitivity no covariates
#| echo: true
data=sensitivity_df.reset_index()
formula = 'win_stay_transformed ~ block_type + valence_diff + arousal_diff + valence_habdiff'
results=smf.mixedlm(formula, data, groups=data['participant_no'], missing='drop').fit(reml=False)
print(results.summary())
```

